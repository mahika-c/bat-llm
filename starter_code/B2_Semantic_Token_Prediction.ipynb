{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# B2 — Interpretable Semantic Token Prediction (Audio to Set of Meaningful Tokens)\n",
        "\n",
        "We convert each labeled example into a small set of **interpretable semantic tokens**:\n",
        "- identity tokens: EMITTER_x, ADDRESSEE_y\n",
        "- context token: CONTEXT_Feeding / CONTEXT_Sleeping / ...\n",
        "- action tokens: E_PRE_Fly_in, E_POST_Stay, A_PRE_Present, A_POST_Fly_away, ...\n",
        "\n",
        "We then train a **multi-label classifier** to predict this token set from acoustic features\n",
        "(AST + token histograms). This creates a \"language-like\" intermediate representation that is:\n",
        "- interpretable\n",
        "- easy to evaluate (token-level F1, per-slot accuracy)\n",
        "- a clean stepping stone to later caption generation (B3)"
      ],
      "metadata": {
        "id": "2y5dOOZlhPd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, classification_report\n"
      ],
      "metadata": {
        "id": "l0NcKjiOhSbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _s(x) -> str:\n",
        "    return str(x).strip()\n",
        "\n",
        "def normalize_slot_value(slot: str, raw) -> str:\n",
        "    \"\"\"Convert numeric codes to human-readable names where applicable.\"\"\"\n",
        "    v = _s(raw)\n",
        "\n",
        "    if slot == \"context\":\n",
        "        return CONTEXT_MAP.get(v, f\"Unknown({v})\")\n",
        "\n",
        "    if slot in (\"emitter_pre\", \"addressee_pre\"):\n",
        "        return PRE_ACTION_MAP.get(v, f\"Unknown({v})\")\n",
        "\n",
        "    if slot in (\"emitter_post\", \"addressee_post\"):\n",
        "        return POST_ACTION_MAP.get(v, f\"Unknown({v})\")\n",
        "\n",
        "    # emitter/addressee: keep IDs\n",
        "    return v\n",
        "\n",
        "def slots_to_tokens(slots: dict) -> list[str]:\n",
        "    \"\"\"\n",
        "    Convert one example’s slots into a set of interpretable tokens.\n",
        "    \"\"\"\n",
        "    emitter = normalize_slot_value(\"emitter\", slots[\"emitter\"])\n",
        "    addressee = normalize_slot_value(\"addressee\", slots[\"addressee\"])\n",
        "    context = normalize_slot_value(\"context\", slots[\"context\"])\n",
        "\n",
        "    e_pre  = normalize_slot_value(\"emitter_pre\", slots[\"emitter_pre\"])\n",
        "    a_pre  = normalize_slot_value(\"addressee_pre\", slots[\"addressee_pre\"])\n",
        "    e_post = normalize_slot_value(\"emitter_post\", slots[\"emitter_post\"])\n",
        "    a_post = normalize_slot_value(\"addressee_post\", slots[\"addressee_post\"])\n",
        "\n",
        "    toks = [\n",
        "        f\"EMITTER_{emitter}\",\n",
        "        f\"ADDRESSEE_{addressee}\",\n",
        "        f\"CONTEXT_{context}\",\n",
        "        f\"E_PRE_{e_pre}\",\n",
        "        f\"A_PRE_{a_pre}\",\n",
        "        f\"E_POST_{e_post}\",\n",
        "        f\"A_POST_{a_post}\",\n",
        "    ]\n",
        "    return toks"
      ],
      "metadata": {
        "id": "QkMRH00hhVuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_token_dataset(labels_raw: dict):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      token_lists: list[list[str]] length N\n",
        "      vocab: list[str]\n",
        "      token_to_id: dict[str,int]\n",
        "      Y: np.ndarray shape (N, V) multi-hot\n",
        "      groups: dict[str, list[int]] token indices grouped by slot-category for per-slot decoding\n",
        "    \"\"\"\n",
        "    N = len(next(iter(labels_raw.values())))\n",
        "    token_lists = []\n",
        "\n",
        "    for i in range(N):\n",
        "        slots_i = {k: labels_raw[k][i] for k in SLOTS}\n",
        "        token_lists.append(slots_to_tokens(slots_i))\n",
        "\n",
        "    # Build vocab\n",
        "    vocab_set = set()\n",
        "    for toks in token_lists:\n",
        "        vocab_set.update(toks)\n",
        "    vocab = sorted(vocab_set)\n",
        "    token_to_id = {t:i for i,t in enumerate(vocab)}\n",
        "\n",
        "    # Multi-hot matrix\n",
        "    V = len(vocab)\n",
        "    Y = np.zeros((N, V), dtype=np.float32)\n",
        "    for i, toks in enumerate(token_lists):\n",
        "        for t in toks:\n",
        "            Y[i, token_to_id[t]] = 1.0\n",
        "\n",
        "    # Group token indices by category (for per-slot decoding)\n",
        "    groups = {\n",
        "        \"emitter\": [token_to_id[t] for t in vocab if t.startswith(\"EMITTER_\")],\n",
        "        \"addressee\": [token_to_id[t] for t in vocab if t.startswith(\"ADDRESSEE_\")],\n",
        "        \"context\": [token_to_id[t] for t in vocab if t.startswith(\"CONTEXT_\")],\n",
        "        \"emitter_pre\": [token_to_id[t] for t in vocab if t.startswith(\"E_PRE_\")],\n",
        "        \"addressee_pre\": [token_to_id[t] for t in vocab if t.startswith(\"A_PRE_\")],\n",
        "        \"emitter_post\": [token_to_id[t] for t in vocab if t.startswith(\"E_POST_\")],\n",
        "        \"addressee_post\": [token_to_id[t] for t in vocab if t.startswith(\"A_POST_\")],\n",
        "    }\n",
        "\n",
        "    return token_lists, vocab, token_to_id, Y, groups\n",
        "\n",
        "token_lists, vocab, token_to_id, Y_all, groups = build_token_dataset(labels_raw)\n",
        "\n",
        "print(\"N examples:\", len(token_lists))\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"Example tokens:\", token_lists[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_zPpK5hbB5",
        "outputId": "9407d142-3bec-4dde-f426-ec5fb65f64e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N examples: 10000\n",
            "Vocab size: 68\n",
            "Example tokens: ['EMITTER_216', 'ADDRESSEE_221', 'CONTEXT_General', 'E_PRE_Present', 'A_PRE_Crawl in', 'E_POST_Stay', 'A_POST_Stay']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pos_weight(Y: np.ndarray, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    pos_weight for BCEWithLogitsLoss:\n",
        "      pos_weight[j] = (N - pos_count[j]) / pos_count[j]\n",
        "    \"\"\"\n",
        "    N = Y.shape[0]\n",
        "    pos = Y.sum(axis=0)\n",
        "    pos = np.clip(pos, eps, None)\n",
        "    neg = N - pos\n",
        "    pw = neg / pos\n",
        "    return torch.tensor(pw, dtype=torch.float32)\n",
        "\n",
        "pos_weight = compute_pos_weight(Y_all)"
      ],
      "metadata": {
        "id": "uT6ZQJrBhbqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X: np.ndarray, Y: np.ndarray):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]"
      ],
      "metadata": {
        "id": "hnELx6TDhe8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class B2aMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims=(512,), dropout=0.2, vocab_size=100):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        d = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(d, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            d = h\n",
        "        self.trunk = nn.Sequential(*layers)\n",
        "        self.out = nn.Linear(d, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.trunk(x)\n",
        "        return self.out(z)  # logits (N, V)"
      ],
      "metadata": {
        "id": "gXQ3pQwghgiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def decode_per_slot_from_logits(logits: np.ndarray, vocab: list[str], groups: dict) -> dict:\n",
        "    \"\"\"\n",
        "    For each slot group, pick the highest-logit token within that group.\n",
        "    Returns: dict slot -> list[str] predicted token strings\n",
        "    \"\"\"\n",
        "    pred = {}\n",
        "    for slot, idxs in groups.items():\n",
        "        sub = logits[:, idxs]  # (N, |group|)\n",
        "        best = np.argmax(sub, axis=1)\n",
        "        pred_tokens = [vocab[idxs[j]] for j in best]\n",
        "        pred[slot] = pred_tokens\n",
        "    return pred\n",
        "\n",
        "def per_slot_accuracy(gt_tokens_by_slot: dict, pred_tokens_by_slot: dict) -> dict:\n",
        "    out = {}\n",
        "    for slot in gt_tokens_by_slot:\n",
        "        gt = np.array(gt_tokens_by_slot[slot], dtype=str)\n",
        "        pr = np.array(pred_tokens_by_slot[slot], dtype=str)\n",
        "        out[slot] = float((gt == pr).mean())\n",
        "    return out\n",
        "\n",
        "def build_gt_tokens_by_slot(token_lists: list[list[str]]) -> dict:\n",
        "    \"\"\"\n",
        "    Extract the single GT token per slot from the 7-token list.\n",
        "    \"\"\"\n",
        "    gt = {k: [] for k in groups.keys()}\n",
        "    for toks in token_lists:\n",
        "        # each example includes exactly one token per slot prefix\n",
        "        for t in toks:\n",
        "            if t.startswith(\"EMITTER_\"):   gt[\"emitter\"].append(t)\n",
        "            elif t.startswith(\"ADDRESSEE_\"): gt[\"addressee\"].append(t)\n",
        "            elif t.startswith(\"CONTEXT_\"): gt[\"context\"].append(t)\n",
        "            elif t.startswith(\"E_PRE_\"):   gt[\"emitter_pre\"].append(t)\n",
        "            elif t.startswith(\"A_PRE_\"):   gt[\"addressee_pre\"].append(t)\n",
        "            elif t.startswith(\"E_POST_\"):  gt[\"emitter_post\"].append(t)\n",
        "            elif t.startswith(\"A_POST_\"):  gt[\"addressee_post\"].append(t)\n",
        "    return gt\n",
        "\n",
        "def train_eval_b2a(\n",
        "    X_all: np.ndarray,\n",
        "    Y_all: np.ndarray,\n",
        "    token_lists: list[list[str]],\n",
        "    vocab: list[str],\n",
        "    groups: dict,\n",
        "    hidden_dims=(512,),\n",
        "    lr=3e-4,\n",
        "    dropout=0.2,\n",
        "    batch_size=256,\n",
        "    epochs=12,\n",
        "    random_state=0,\n",
        "    stratify_on_context=True,\n",
        "):\n",
        "    # Split (optionally stratify by context token for stability)\n",
        "    gt_by_slot = build_gt_tokens_by_slot(token_lists)\n",
        "    strat = None\n",
        "    if stratify_on_context:\n",
        "        strat = np.array(gt_by_slot[\"context\"], dtype=str)\n",
        "\n",
        "    X_tr, X_te, Y_tr, Y_te, toks_tr, toks_te = train_test_split(\n",
        "        X_all, Y_all, token_lists,\n",
        "        test_size=0.2, random_state=random_state, stratify=strat\n",
        "    )\n",
        "\n",
        "    # Scale like A-models\n",
        "    scaler = StandardScaler()\n",
        "    X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
        "    X_te = scaler.transform(X_te).astype(np.float32)\n",
        "\n",
        "    # DataLoaders\n",
        "    tr_ds = TokenDataset(X_tr, Y_tr)\n",
        "    te_ds = TokenDataset(X_te, Y_te)\n",
        "    tr_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
        "    te_loader = torch.utils.data.DataLoader(te_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = B2aMLP(input_dim=X_all.shape[1], hidden_dims=hidden_dims, dropout=dropout, vocab_size=len(vocab)).to(device)\n",
        "\n",
        "    # pos_weight for imbalance\n",
        "    pos_weight = compute_pos_weight(Y_tr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for xb, yb in tr_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total += loss.item() * xb.size(0)\n",
        "\n",
        "        avg_loss = total / len(tr_ds)\n",
        "        if ep == 1 or ep == epochs or ep % 3 == 0:\n",
        "            print(f\"epoch {ep:02d}/{epochs}  train_loss={avg_loss:.4f}\")\n",
        "\n",
        "    # Eval\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_Y = []\n",
        "    for xb, yb in te_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb).detach().cpu().numpy()\n",
        "        all_logits.append(logits)\n",
        "        all_Y.append(yb.numpy())\n",
        "\n",
        "    logits_te = np.vstack(all_logits)\n",
        "    Y_te = np.vstack(all_Y)\n",
        "\n",
        "    # Multi-label token F1 (threshold at 0.5 on sigmoid)\n",
        "    probs = 1 / (1 + np.exp(-logits_te))\n",
        "    Y_hat = (probs >= 0.5).astype(int)\n",
        "\n",
        "    micro = f1_score(Y_te, Y_hat, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(Y_te, Y_hat, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Per-slot accuracy via argmax within each group\n",
        "    pred_by_slot = decode_per_slot_from_logits(logits_te, vocab, groups)\n",
        "\n",
        "    # Build GT by slot for *test* subset\n",
        "    gt_te_by_slot = build_gt_tokens_by_slot(toks_te)\n",
        "    slot_acc = per_slot_accuracy(gt_te_by_slot, pred_by_slot)\n",
        "\n",
        "    out = {\n",
        "        \"model\": model,\n",
        "        \"scaler\": scaler,\n",
        "        \"micro_f1_tokens\": float(micro),\n",
        "        \"macro_f1_tokens\": float(macro),\n",
        "        \"slot_accuracy\": slot_acc,\n",
        "        \"vocab\": vocab,\n",
        "        \"groups\": groups,\n",
        "    }\n",
        "    return out"
      ],
      "metadata": {
        "id": "oeWo_njdhjLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "    {\"hidden_dims\": (512,), \"lr\": 3e-4},\n",
        "    {\"hidden_dims\": (512,), \"lr\": 1e-3},\n",
        "    {\"hidden_dims\": (512, 256), \"lr\": 3e-4},\n",
        "    {\"hidden_dims\": (512, 256), \"lr\": 1e-3},\n",
        "]\n",
        "\n",
        "results = []\n",
        "for cfg in configs:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"B2a config:\", cfg)\n",
        "    out_b2a = train_eval_b2a(\n",
        "        X_all=X_all,\n",
        "        Y_all=Y_all,\n",
        "        token_lists=token_lists,\n",
        "        vocab=vocab,\n",
        "        groups=groups,\n",
        "        hidden_dims=cfg[\"hidden_dims\"],\n",
        "        lr=cfg[\"lr\"],\n",
        "        dropout=0.2,\n",
        "        epochs=12,\n",
        "        batch_size=256,\n",
        "        random_state=0,\n",
        "        stratify_on_context=True,\n",
        "    )\n",
        "    print(f\"Token micro-F1: {out_b2a['micro_f1_tokens']:.4f} | macro-F1: {out_b2a['macro_f1_tokens']:.4f}\")\n",
        "    print(\"Per-slot accuracy:\")\n",
        "    for k,v in out_b2a[\"slot_accuracy\"].items():\n",
        "        print(f\"  {k:14s}: {v:.4f}\")\n",
        "    results.append((cfg, out_b2a))\n",
        "\n",
        "# Pick best by context slot accuracy (mirrors your A2 selection logic)\n",
        "best_cfg, best_out = max(results, key=lambda x: x[1][\"slot_accuracy\"][\"context\"])\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"BEST (by context slot accuracy):\", best_cfg)\n",
        "print(\"Best per-slot accuracy:\", best_out[\"slot_accuracy\"])\n",
        "print(f\"Best token micro-F1={best_out['micro_f1_tokens']:.4f}, macro-F1={best_out['macro_f1_tokens']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8atvCQw0hkSI",
        "outputId": "ca6a8d30-7b64-477e-8486-8c2798bc62c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "B2a config: {'hidden_dims': (512,), 'lr': 0.0003}\n",
            "epoch 01/12  train_loss=1.1372\n",
            "epoch 03/12  train_loss=0.7576\n",
            "epoch 06/12  train_loss=0.6111\n",
            "epoch 09/12  train_loss=0.5390\n",
            "epoch 12/12  train_loss=0.4912\n",
            "Token micro-F1: 0.5400 | macro-F1: 0.3218\n",
            "Per-slot accuracy:\n",
            "  emitter       : 0.6275\n",
            "  addressee     : 0.4875\n",
            "  context       : 0.5715\n",
            "  emitter_pre   : 0.7030\n",
            "  addressee_pre : 0.7265\n",
            "  emitter_post  : 0.7130\n",
            "  addressee_post: 0.7165\n",
            "\n",
            "================================================================================\n",
            "B2a config: {'hidden_dims': (512,), 'lr': 0.001}\n",
            "epoch 01/12  train_loss=1.1050\n",
            "epoch 03/12  train_loss=0.6078\n",
            "epoch 06/12  train_loss=0.4789\n",
            "epoch 09/12  train_loss=0.4095\n",
            "epoch 12/12  train_loss=0.3588\n",
            "Token micro-F1: 0.5870 | macro-F1: 0.3439\n",
            "Per-slot accuracy:\n",
            "  emitter       : 0.6625\n",
            "  addressee     : 0.4605\n",
            "  context       : 0.5705\n",
            "  emitter_pre   : 0.7330\n",
            "  addressee_pre : 0.7230\n",
            "  emitter_post  : 0.7405\n",
            "  addressee_post: 0.7195\n",
            "\n",
            "================================================================================\n",
            "B2a config: {'hidden_dims': (512, 256), 'lr': 0.0003}\n",
            "epoch 01/12  train_loss=1.1908\n",
            "epoch 03/12  train_loss=0.8441\n",
            "epoch 06/12  train_loss=0.6565\n",
            "epoch 09/12  train_loss=0.5774\n",
            "epoch 12/12  train_loss=0.5261\n",
            "Token micro-F1: 0.5219 | macro-F1: 0.3156\n",
            "Per-slot accuracy:\n",
            "  emitter       : 0.6015\n",
            "  addressee     : 0.4700\n",
            "  context       : 0.5560\n",
            "  emitter_pre   : 0.7015\n",
            "  addressee_pre : 0.7195\n",
            "  emitter_post  : 0.6995\n",
            "  addressee_post: 0.7060\n",
            "\n",
            "================================================================================\n",
            "B2a config: {'hidden_dims': (512, 256), 'lr': 0.001}\n",
            "epoch 01/12  train_loss=1.0861\n",
            "epoch 03/12  train_loss=0.6967\n",
            "epoch 06/12  train_loss=0.5387\n",
            "epoch 09/12  train_loss=0.4638\n",
            "epoch 12/12  train_loss=0.4133\n",
            "Token micro-F1: 0.5607 | macro-F1: 0.3372\n",
            "Per-slot accuracy:\n",
            "  emitter       : 0.6505\n",
            "  addressee     : 0.4560\n",
            "  context       : 0.5225\n",
            "  emitter_pre   : 0.7180\n",
            "  addressee_pre : 0.7390\n",
            "  emitter_post  : 0.7140\n",
            "  addressee_post: 0.7030\n",
            "\n",
            "################################################################################\n",
            "BEST (by context slot accuracy): {'hidden_dims': (512,), 'lr': 0.0003}\n",
            "Best per-slot accuracy: {'emitter': 0.6275, 'addressee': 0.4875, 'context': 0.5715, 'emitter_pre': 0.703, 'addressee_pre': 0.7265, 'emitter_post': 0.713, 'addressee_post': 0.7165}\n",
            "Best token micro-F1=0.5400, macro-F1=0.3218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_b2a_tokens(best_out: dict, X: np.ndarray, topk_per_group: int = 1):\n",
        "    model = best_out[\"model\"]\n",
        "    scaler = best_out[\"scaler\"]\n",
        "    vocab = best_out[\"vocab\"]\n",
        "    groups = best_out[\"groups\"]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    Xs = scaler.transform(X).astype(np.float32)\n",
        "    xb = torch.tensor(Xs, dtype=torch.float32, device=device)\n",
        "    logits = model(xb).detach().cpu().numpy()\n",
        "\n",
        "    pred_by_slot = decode_per_slot_from_logits(logits, vocab, groups)\n",
        "    # combine into a \"token caption\"\n",
        "    captions = []\n",
        "    for i in range(len(X)):\n",
        "        caps = [\n",
        "            pred_by_slot[\"emitter\"][i],\n",
        "            pred_by_slot[\"addressee\"][i],\n",
        "            pred_by_slot[\"context\"][i],\n",
        "            pred_by_slot[\"emitter_pre\"][i],\n",
        "            pred_by_slot[\"addressee_pre\"][i],\n",
        "            pred_by_slot[\"emitter_post\"][i],\n",
        "            pred_by_slot[\"addressee_post\"][i],\n",
        "        ]\n",
        "        captions.append(\" \".join(caps))\n",
        "    return captions\n",
        "\n",
        "b2a_token_captions = predict_b2a_tokens(best_out, X_all)\n",
        "print(\"Example B2a token caption:\\n\", b2a_token_captions[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoMCprvmhnDi",
        "outputId": "4bfc5802-6013-4881-f5df-dad4d4a99d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example B2a token caption:\n",
            " EMITTER_111 ADDRESSEE_221 CONTEXT_Fighting E_PRE_Present A_PRE_Present E_POST_Fly away A_POST_Stay\n"
          ]
        }
      ]
    }
  ]
}