{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08 â€“ Next-Token Language Model over Acoustic Tokens\n",
        "\n",
        "Trains a **next-token prediction model** (a small Transformer language model)\n",
        "over discrete acoustic token sequences derived in `05_Tokenization_Strategies.ipynb`.\n",
        "\n",
        "treat each file's token sequence (ex. wav2vec2+k-means or VQ-VAE codes) as a \"sentence\",\n",
        "train the model to predict token \\(t_{i+1}\\) given the history \\(t_{\\le i}\\)\n",
        "\n",
        "Reports:\n",
        "- **Validation cross-entropy loss**\n",
        "- **Perplexity** \\(= e^{\\text{loss}}\\)\n",
        "- **Token-level accuracy** on the validation set.\n",
        "\n",
        "Can also generate sample continuations from the trained model to qualitatively inspect behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "ROOT: /Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code\n",
            "Using tokenization: kmeans\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "ROOT = Path.cwd().resolve()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "DERIVED_DIR = ROOT / \"derived\"\n",
        "TOKENS_DIR = DERIVED_DIR / \"tokens\"\n",
        "KMEANS_DIR = TOKENS_DIR / \"k_means\"\n",
        "VQ_TOKENS_DIR = TOKENS_DIR / \"vqvae\"\n",
        "\n",
        "# choose which tokenization to use for the LM: \"kmeans\" or \"vqvae\"\n",
        "TOKEN_TYPE = \"kmeans\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"Using tokenization:\", TOKEN_TYPE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load token sequences\n",
        "\n",
        "We load per-file token sequences from `derived/tokens/`.\n",
        "\n",
        "- For `TOKEN_TYPE == \"kmeans\"`, we use `w2v_kmeans_<stem>.npy` (K=128 clusters).\n",
        "- For `TOKEN_TYPE == \"vqvae\"`, we use `vqvae_<stem>.npy` (default 256 codes).\n",
        "\n",
        "Each file contributes a 1D integer sequence of token IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10000 sequences from /Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code/derived/tokens/k_means with vocab_size=128.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(10000, 128)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_token_sequences(token_type: str = TOKEN_TYPE) -> Tuple[List[np.ndarray], int]:\n",
        "    \"\"\"Load all available token sequences and infer vocabulary size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sequences : list of np.ndarray\n",
        "        Each entry is a 1D int array of token IDs for one file.\n",
        "    vocab_size : int\n",
        "        Maximum token ID + 1 across all sequences (upper bound on vocabulary).\n",
        "    \"\"\"\n",
        "\n",
        "    sequences: List[np.ndarray] = []\n",
        "    max_token_id = -1\n",
        "\n",
        "    if token_type == \"kmeans\":\n",
        "        pattern = \"w2v_kmeans_\"\n",
        "        base_dir = KMEANS_DIR\n",
        "    elif token_type == \"vqvae\":\n",
        "        pattern = \"vqvae_\"\n",
        "        base_dir = VQ_TOKENS_DIR\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported TOKEN_TYPE: {token_type}\")\n",
        "\n",
        "    if not base_dir.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Token directory {base_dir} not found. Run 05_Tokenization_Strategies.ipynb first.\"\n",
        "        )\n",
        "\n",
        "    paths = sorted(base_dir.glob(\"*.npy\"))\n",
        "    if not paths:\n",
        "        raise RuntimeError(f\"No token files found in {base_dir}.\")\n",
        "\n",
        "    for p in paths:\n",
        "        if pattern not in p.name:\n",
        "            continue\n",
        "        arr = np.load(p).astype(np.int64)\n",
        "        if arr.ndim != 1 or arr.size < 2:\n",
        "            continue\n",
        "        max_token_id = max(max_token_id, int(arr.max()))\n",
        "        sequences.append(arr)\n",
        "\n",
        "    if not sequences:\n",
        "        raise RuntimeError(f\"No valid token sequences loaded from {base_dir}.\")\n",
        "\n",
        "    vocab_size = max_token_id + 1\n",
        "    print(f\"Loaded {len(sequences)} sequences from {base_dir} with vocab_size={vocab_size}.\")\n",
        "    return sequences, vocab_size\n",
        "\n",
        "\n",
        "sequences, vocab_size = load_token_sequences(TOKEN_TYPE)\n",
        "len(sequences), vocab_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset for next-token prediction\n",
        "\n",
        "We turn each token sequence into many training examples of length `seq_len`:\n",
        "\n",
        "- Input: tokens `[t_i, ..., t_{i+L-1}]`\n",
        "- Target: tokens `[t_{i+1}, ..., t_{i+L}]`\n",
        "\n",
        "We step with a configurable `stride` to control how many overlapping windows we create.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: 71935 windows, seq_len=64, stride=4.\n",
            "Dataset: 7637 windows, seq_len=64, stride=4.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(71935, 7637)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequences: List[np.ndarray],\n",
        "        seq_len: int = 64,\n",
        "        stride: int = 4,\n",
        "    ) -> None:\n",
        "        self.sequences = sequences\n",
        "        self.seq_len = seq_len\n",
        "        self.stride = max(1, stride)\n",
        "\n",
        "        # precompute (sequence_idx, start) pairs\n",
        "        indices: List[Tuple[int, int]] = []\n",
        "        for si, seq in enumerate(sequences):\n",
        "            n = len(seq)\n",
        "            if n <= seq_len:\n",
        "                continue\n",
        "            # last start index that allows target to go up to t_{i+L}\n",
        "            max_start = n - (seq_len + 1)\n",
        "            if max_start < 0:\n",
        "                continue\n",
        "            for start in range(0, max_start + 1, self.stride):\n",
        "                indices.append((si, start))\n",
        "\n",
        "        if not indices:\n",
        "            raise RuntimeError(\"No training windows could be formed. Try shorter seq_len or stride.\")\n",
        "\n",
        "        self.indices = indices\n",
        "        print(f\"Dataset: {len(self.indices)} windows, seq_len={self.seq_len}, stride={self.stride}.\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        si, start = self.indices[idx]\n",
        "        seq = self.sequences[si]\n",
        "        x = torch.from_numpy(seq[start : start + self.seq_len]).long()\n",
        "        y = torch.from_numpy(seq[start + 1 : start + 1 + self.seq_len]).long()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "SEQ_LEN = 64\n",
        "STRIDE = 4\n",
        "\n",
        "# train/val split at the sequence level to avoid leakage\n",
        "num_sequences = len(sequences)\n",
        "val_frac = 0.1\n",
        "num_val = max(1, int(num_sequences * val_frac))\n",
        "num_train = num_sequences - num_val\n",
        "\n",
        "all_indices = list(range(num_sequences))\n",
        "rng = np.random.default_rng(42)\n",
        "rng.shuffle(all_indices)\n",
        "train_indices = all_indices[:num_train]\n",
        "val_indices = all_indices[num_train:]\n",
        "\n",
        "train_seqs = [sequences[i] for i in train_indices]\n",
        "val_seqs = [sequences[i] for i in val_indices]\n",
        "\n",
        "train_dataset = NextTokenDataset(train_seqs, seq_len=SEQ_LEN, stride=STRIDE)\n",
        "val_dataset = NextTokenDataset(val_seqs, seq_len=SEQ_LEN, stride=STRIDE)\n",
        "\n",
        "len(train_dataset), len(val_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer language model\n",
        "\n",
        "Define a small Transformer-based language model that operates on token sequences.\n",
        "It uses learned token and positional embeddings plus a stack of Transformer encoder layers\n",
        "with a causal mask so each position can only attend to previous positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerLM(\n",
              "  (token_emb): Embedding(128, 256)\n",
              "  (pos_emb): Embedding(64, 256)\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=256, out_features=128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 256,\n",
        "        n_heads: int = 4,\n",
        "        num_layers: int = 4,\n",
        "        dim_feedforward: int = 512,\n",
        "        max_seq_len: int = 256,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def _generate_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
        "        # (seq_len, seq_len) with True in upper-right triangle (masked positions)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : LongTensor of shape (batch, seq_len)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        logits : FloatTensor of shape (batch, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "\n",
        "        bsz, seq_len = x.shape\n",
        "        if seq_len > self.max_seq_len:\n",
        "            raise ValueError(f\"seq_len={seq_len} exceeds max_seq_len={self.max_seq_len}\")\n",
        "\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(bsz, seq_len)\n",
        "        h = self.token_emb(x) + self.pos_emb(positions)\n",
        "\n",
        "        src_mask = self._generate_causal_mask(seq_len, device=x.device)\n",
        "        h = self.encoder(h, mask=src_mask)\n",
        "        h = self.ln_f(h)\n",
        "        logits = self.head(h)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = TransformerLM(vocab_size=vocab_size, d_model=256, n_heads=4, num_layers=4, dim_feedforward=512, max_seq_len=SEQ_LEN).to(device)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluation\n",
        "\n",
        "We train the model to minimize cross-entropy loss on the next-token prediction task.\n",
        "After each epoch we compute validation loss, perplexity (exp of loss), and\n",
        "token-level accuracy on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 5\n",
        "LR = 3e-4\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(x)  # (B, L, V)\n",
        "            B, L, V = logits.shape\n",
        "            logits_flat = logits.view(B * L, V)\n",
        "            y_flat = y.view(B * L)\n",
        "\n",
        "            loss = criterion(logits_flat, y_flat)\n",
        "            total_loss += loss.item() * (B * L)\n",
        "            total_tokens += B * L\n",
        "\n",
        "            preds = logits_flat.argmax(dim=-1)\n",
        "            correct += (preds == y_flat).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_tokens)\n",
        "    acc = correct / max(1, total_tokens)\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_tokens = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        B, L, V = logits.shape\n",
        "        loss = criterion(logits.view(B * L, V), y.view(B * L))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * (B * L)\n",
        "        running_tokens += B * L\n",
        "\n",
        "    train_loss = running_loss / max(1, running_tokens)\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "    print(f\"Epoch {epoch}/{N_EPOCHS} - train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_ppl={val_ppl:.2f} | val_acc={val_acc:.4f}\")\n",
        "\n",
        "# can load best model state\n",
        "if 'best_state' in globals():\n",
        "    model.load_state_dict(best_state)\n",
        "    model.to(device)\n",
        "    print(\"Loaded best model state (by val loss).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling / generation\n",
        "\n",
        "We can generate new token sequences by autoregressively sampling from the model's\n",
        "predicted distribution, starting from an initial context window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_tokens(\n",
        "    model: nn.Module,\n",
        "    start_tokens: torch.Tensor,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int | None = 50,\n",
        "    max_seq_len: int = 256,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Generate a continuation of tokens from a starting context.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_tokens : LongTensor of shape (seq_len,)\n",
        "        Initial context tokens.\n",
        "    max_seq_len : int\n",
        "        Maximum context length to feed into the model.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    generated = start_tokens.clone().to(device).unsqueeze(0)  # (1, T)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Ensure we respect the model's maximum sequence length\n",
        "        if generated.size(1) > max_seq_len:\n",
        "            context = generated[:, -max_seq_len :]\n",
        "        else:\n",
        "            context = generated\n",
        "\n",
        "        logits = model(context)  # (1, T, V)\n",
        "        logits = logits[:, -1, :] / max(1e-6, temperature)  # (1, V)\n",
        "\n",
        "        k = top_k\n",
        "        if k is not None and k > 0:\n",
        "            k = min(k, int(logits.size(-1)))\n",
        "            values, indices = torch.topk(logits, k)\n",
        "            probs = torch.zeros_like(logits).scatter_(-1, indices, values)\n",
        "            probs = torch.softmax(probs, dim=-1)\n",
        "        else:\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
        "        generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "    return generated.squeeze(0).cpu()\n",
        "\n",
        "\n",
        "# EX: take a random validation sequence and generate a continuation\n",
        "example_seq = val_seqs[0]\n",
        "start = example_seq[:SEQ_LEN]\n",
        "\n",
        "generated = generate_tokens(\n",
        "    model,\n",
        "    torch.from_numpy(start).long(),\n",
        "    max_new_tokens=50,\n",
        "    temperature=1.0,\n",
        "    top_k=50,\n",
        "    max_seq_len=SEQ_LEN,\n",
        ")\n",
        "\n",
        "generated[:SEQ_LEN], generated[SEQ_LEN:]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
