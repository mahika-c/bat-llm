{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Strategies for Bat Vocalizations\n",
    "\n",
    "This notebook sets up three representation / tokenization strategies on top of the 10k subset (using `data/annotations.csv` and derived features):\n",
    "\n",
    "1. **Self-supervised speech encoders (wav2vec 2.0 / HuBERT) + k-means** to produce discrete \"bio-tokens\".\n",
    "2. **VQ-VAE on mel-spectrograms** to learn a discrete codebook of acoustic units.\n",
    "3. **Continuous-feature encoders (AST)** that operate on spectrograms without discretization.\n",
    "\n",
    "Classifier or sequence models can be trained later on top of the saved representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ROOT = Path(__file__).resolve().parent\n",
    "DATA_DIR = ROOT / 'data'\n",
    "DERIVED_DIR = ROOT / 'derived'\n",
    "AUDIO_DIR = DATA_DIR / 'audio'\n",
    "MELS_48K_DIR = DERIVED_DIR / 'mels_48k'\n",
    "TOKENS_DIR = DERIVED_DIR / 'tokens'\n",
    "AST_DIR = DERIVED_DIR / 'ast_features'\n",
    "\n",
    "TOKENS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "AST_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. wav2vec 2.0 / HuBERT embeddings + k-means clustering\n",
    "\n",
    "We use a pretrained self-supervised speech model to get frame-level embeddings, then learn a k-means codebook to derive discrete token sequences per call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import librosa\n",
    "\n",
    "W2V_MODEL_NAME = 'facebook/wav2vec2-base'  # or a HuBERT variant\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(W2V_MODEL_NAME)\n",
    "w2v_model = Wav2Vec2Model.from_pretrained(W2V_MODEL_NAME).to(device).eval()\n",
    "\n",
    "ann_small = pd.read_csv(DATA_DIR / 'annotations.csv')\n",
    "\n",
    "def load_audio_for_w2v(path: Path, target_sr: int = 16_000) -> np.ndarray:\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    if sr != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def extract_w2v_embeddings(wav: np.ndarray, sr: int = 16_000) -> np.ndarray:\n",
    "    inputs = feature_extractor(wav, sampling_rate=sr, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        out = w2v_model(inputs.input_values.to(device))\n",
    "    # shape: (1, T, hidden_size)\n",
    "    return out.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Example: extract embeddings for a small subset of files\n",
    "subset = ann_small['File Name'].iloc[:128]\n",
    "all_frames: List[np.ndarray] = []\n",
    "file2frame_indices: Dict[str, slice] = {}\n",
    "start = 0\n",
    "for fn in subset:\n",
    "    path = AUDIO_DIR / fn\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    wav = load_audio_for_w2v(path)\n",
    "    emb = extract_w2v_embeddings(wav)  # (T, D)\n",
    "    end = start + emb.shape[0]\n",
    "    all_frames.append(emb)\n",
    "    file2frame_indices[fn] = slice(start, end)\n",
    "    start = end\n",
    "\n",
    "frame_matrix = np.concatenate(all_frames, axis=0)  # (total_T, D)\n",
    "frame_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit k-means on frame-level embeddings to create a codebook\n",
    "N_CLUSTERS = 128\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0)\n",
    "kmeans.fit(frame_matrix)\n",
    "\n",
    "# Convert each file's frames into a sequence of discrete token IDs\n",
    "file2tokens: Dict[str, np.ndarray] = {}\n",
    "for fn, sl in file2frame_indices.items():\n",
    "    frame_embs = frame_matrix[sl]\n",
    "    tokens = kmeans.predict(frame_embs)\n",
    "    file2tokens[fn] = tokens.astype(np.int16)\n",
    "    np.save(TOKENS_DIR / f'w2v_kmeans_{Path(fn).stem}.npy', tokens)\n",
    "\n",
    "len(file2tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VQ-VAE over mel-spectrograms\n",
    "\n",
    "We now sketch a simple VQ-VAE model that operates on log-mel spectrogram patches loaded from `derived/mels_48k`.\n",
    "This code is a starting point; you can tune architecture and training hyperparameters based on resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_codes: int, code_dim: int, beta: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.code_dim = code_dim\n",
    "        self.embeddings = nn.Embedding(num_codes, code_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1.0 / num_codes, 1.0 / num_codes)\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        # z: (B, C, T, F) -> flatten to (B*T*F, C)\n",
    "        z_perm = z.permute(0, 2, 3, 1).contiguous()\n",
    "        flat_z = z_perm.view(-1, self.code_dim)\n",
    "        # Compute distances to codebook\n",
    "        distances = (\n",
    "            flat_z.pow(2).sum(dim=1, keepdim=True)\n",
    "            - 2 * flat_z @ self.embeddings.weight.t()\n",
    "            + self.embeddings.weight.pow(2).sum(dim=1)\n",
    "        )\n",
    "        codes = distances.argmin(dim=1)\n",
    "        z_q = self.embeddings(codes).view(*z_perm.shape)\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "        # VQ-VAE losses\n",
    "        commitment_loss = self.beta * (z_q.detach() - z).pow(2).mean()\n",
    "        codebook_loss = (z_q - z.detach()).pow(2).mean()\n",
    "        z_q = z + (z_q - z).detach()  # straight-through\n",
    "        return z_q, codes.view(z.size(0), -1), commitment_loss + codebook_loss\n",
    "\n",
    "class SimpleVQVAE(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, num_codes: int = 256):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.vq = VectorQuantizer(num_codes=num_codes, code_dim=hidden_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(hidden_dim, 1, kernel_size=4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):  # x: (B, 1, T, F)\n",
    "        z = self.encoder(x)\n",
    "        z_q, codes, vq_loss = self.vq(z)\n",
    "        recon = self.decoder(z_q)\n",
    "        recon_loss = (x - recon).pow(2).mean()\n",
    "        return recon, codes, recon_loss + vq_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal data loader for mel patches\n",
    "class MelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mels_dir: Path, file_names: List[str]):\n",
    "        self.mel_paths = [mels_dir / (Path(fn).stem + '.npy') for fn in file_names]\n",
    "        self.mel_paths = [p for p in self.mel_paths if p.exists()]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mel_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        mel = np.load(self.mel_paths[idx])  # (n_mels, T)\n",
    "        x = torch.from_numpy(mel.T).unsqueeze(0)  # (1, T, F)\n",
    "        return x\n",
    "\n",
    "subset_fns = ann_small['File Name'].iloc[:256].tolist()\n",
    "mel_ds = MelDataset(MELS_48K_DIR, subset_fns)\n",
    "mel_dl = torch.utils.data.DataLoader(mel_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "vqvae = SimpleVQVAE().to(device)\n",
    "opt = torch.optim.Adam(vqvae.parameters(), lr=1e-3)\n",
    "\n",
    "# Very small warm-up training loop (extend as needed)\n",
    "for step, x in enumerate(mel_dl):\n",
    "    x = x.to(device)\n",
    "    recon, codes, loss = vqvae(x)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step % 10 == 0:\n",
    "        print(f'step {step}: loss={loss.item():.4f}')\n",
    "    if step >= 50:  # keep short by default\n",
    "        break\n",
    "\n",
    "# Save code indices for a few examples\n",
    "vq_tokens_dir = TOKENS_DIR / 'vqvae'\n",
    "vq_tokens_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "vqvae.eval()\n",
    "with torch.no_grad():\n",
    "    for fn in subset_fns[:64]:\n",
    "        mel_path = MELS_48K_DIR / (Path(fn).stem + '.npy')\n",
    "        if not mel_path.exists():\n",
    "            continue\n",
    "        mel = np.load(mel_path).T  # (T, F)\n",
    "        x = torch.from_numpy(mel).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        _, codes, _ = vqvae(x)\n",
    "        codes_np = codes.squeeze(0).cpu().numpy().astype(np.int16)\n",
    "        np.save(vq_tokens_dir / f'vqvae_{Path(fn).stem}.npy', codes_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous representations with AST (Audio Spectrogram Transformer)\n",
    "\n",
    "We obtain continuous embeddings from an Audio Spectrogram Transformer (AST), e.g., pretrained on AudioSet.\n",
    "These can be used directly for downstream classification or captioning without discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTModel\n",
    "\n",
    "AST_MODEL_NAME = 'MIT/ast-finetuned-audioset-10-10-0.4593'\n",
    "ast_extractor = AutoFeatureExtractor.from_pretrained(AST_MODEL_NAME)\n",
    "ast_model = ASTModel.from_pretrained(AST_MODEL_NAME).to(device).eval()\n",
    "\n",
    "def load_mel_as_ast_input(mel_path: Path) -> Dict[str, torch.Tensor]:\n",
    "    mel = np.load(mel_path)  # (n_mels, T)\n",
    "    inputs = ast_extractor(\n",
    "        mel,\n",
    "        sampling_rate=48_000,  # nominal value when passing features directly\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "for fn in ann_small['File Name'].iloc[:128]:\n",
    "    mel_path = MELS_48K_DIR / (Path(fn).stem + '.npy')\n",
    "    if not mel_path.exists():\n",
    "        continue\n",
    "    inputs = load_mel_as_ast_input(mel_path)\n",
    "    with torch.no_grad():\n",
    "        out = ast_model(**inputs)\n",
    "    pooled = out.pooler_output.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "    np.save(AST_DIR / f'ast_{Path(fn).stem}.npy', pooled)\n",
    "\n",
    "len(list(AST_DIR.glob('*.npy')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
