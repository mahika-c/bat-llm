{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 – Baseline Classification with AST Features\n",
        "\n",
        "This notebook builds simple baseline classifiers for the bat vocalization dataset,\n",
        "using pre-computed **AST embeddings** from `05_Tokenization_Strategies.ipynb` and\n",
        "labels from `annotations_10k.csv`.\n",
        "\n",
        "We train logistic-regression baselines for:\n",
        "- **Emitter classification** (which bat emitted the call)\n",
        "- **Context classification** (behavioral context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code'),\n",
              " PosixPath('/Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code/derived/ast_features'),\n",
              " PosixPath('/Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code/annotations_10k.csv'))"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Paths (assume this notebook is run from starter_code/)\n",
        "ROOT = Path.cwd().resolve()\n",
        "DERIVED_DIR = ROOT / 'derived'\n",
        "AST_DIR = DERIVED_DIR / 'ast_features'\n",
        "ANNOT_PATH = ROOT / 'annotations_10k.csv'\n",
        "\n",
        "ROOT, AST_DIR, ANNOT_PATH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load annotations\n",
        "\n",
        "We load the 10k annotations file and sanity-check that the key columns exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_annotations(path: Path) -> pd.DataFrame:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f'annotations file not found at {path}')\n",
        "    ann = pd.read_csv(path)\n",
        "    # Expect at least these columns:\n",
        "    # Emitter, File Name, FileID, Addressee, Context,\n",
        "    # Emitter pre-vocalization action, Addressee pre-vocalization action,\n",
        "    # Emitter post-vocalization action, Addressee post-vocalization action,\n",
        "    # Start sample, End sample\n",
        "    required_cols = ['Emitter', 'File Name', 'Context']\n",
        "    missing = [c for c in required_cols if c not in ann.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f'annotations_10k.csv missing columns: {missing}')\n",
        "    return ann\n",
        "\n",
        "ann = load_annotations(ANNOT_PATH)\n",
        "ann.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect AST features\n",
        "\n",
        "Each file has a pooled AST embedding saved as `derived/ast_features/ast_<stem>.npy`.\n",
        "We load these and align them with the labels from the annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_ast_features(ann: pd.DataFrame) -> tuple[np.ndarray, list[str], list[str]]:\n",
        "    \"\"\"Load one AST embedding per file and align with labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : np.ndarray, shape (N, D)\n",
        "        AST pooled embeddings.\n",
        "    emitters : list[str]\n",
        "        Emitter labels per example.\n",
        "    contexts : list[str]\n",
        "        Context labels per example.\n",
        "    \"\"\"\n",
        "\n",
        "    stems = ann['File Name'].apply(lambda s: Path(str(s)).stem)\n",
        "\n",
        "    X_list: list[np.ndarray] = []\n",
        "    emitters: list[str] = []\n",
        "    contexts: list[str] = []\n",
        "\n",
        "    missing = 0\n",
        "    for fn, stem, emitter, ctx in zip(ann['File Name'], stems, ann['Emitter'], ann['Context']):\n",
        "        ast_path = AST_DIR / f'ast_{stem}.npy'\n",
        "        if not ast_path.exists():\n",
        "            missing += 1\n",
        "            continue\n",
        "        vec = np.load(ast_path)\n",
        "        # Expect 1D vector; ensure that.\n",
        "        vec = np.asarray(vec, dtype=np.float32).reshape(-1)\n",
        "        X_list.append(vec)\n",
        "        emitters.append(str(emitter))\n",
        "        contexts.append(str(ctx))\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\n",
        "            'No AST feature files found. Make sure 05_Tokenization_Strategies.ipynb '\n",
        "            'has been run to generate derived/ast_features/ast_*.npy.'\n",
        "        )\n",
        "\n",
        "    X = np.vstack(X_list)\n",
        "    print(\n",
        "        f'Loaded AST embeddings for {X.shape[0]} examples '\n",
        "        f'(skipped {missing} without features); dim={X.shape[1]}'\n",
        "    )\n",
        "    return X, emitters, contexts\n",
        "\n",
        "X_ast, y_emitters, y_contexts = collect_ast_features(ann)\n",
        "X_ast.shape, len(y_emitters), len(y_contexts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper to train a simple baseline\n",
        "\n",
        "We use logistic regression on top of standardized AST features and report standard\n",
        "classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_baseline(\n",
        "    X: np.ndarray,\n",
        "    y: list[str],\n",
        "    task_name: str,\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 42,\n",
        ") -> None:\n",
        "    \"\"\"Train a simple logistic-regression baseline and print metrics.\"\"\"\n",
        "\n",
        "    y_arr = np.asarray(y, dtype=object)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y_arr)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y_enc,\n",
        "        test_size=test_size,\n",
        "        stratify=y_enc,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
        "    clf.fit(X_train_s, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test_s)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Baseline: {task_name} (AST + logistic regression)\")\n",
        "    print(\"Classes:\", list(le.classes_))\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "    print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline 1 – Emitter classification\n",
        "\n",
        "Predict which bat (emitter) produced each call from its AST embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_baseline(X_ast, y_emitters, task_name='Emitter classification')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline 2 – Context classification\n",
        "\n",
        "Predict the behavioral context label from the same AST embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(set(y_contexts)) > 1:\n",
        "    run_baseline(X_ast, y_contexts, task_name='Context classification')\n",
        "else:\n",
        "    print('[info] Skipping context baseline: only one unique context label found.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
