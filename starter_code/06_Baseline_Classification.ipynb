{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 – Baseline Classification with AST Features\n",
        "\n",
        "This notebook builds simple baseline classifiers for the bat vocalization dataset,\n",
        "using pre-computed **AST embeddings** from `05_Tokenization_Strategies.ipynb` and\n",
        "labels from `annotations_10k.csv`.\n",
        "\n",
        "We train logistic-regression baselines for:\n",
        "- **Emitter classification** (which bat emitted the call)\n",
        "- **Context classification** (behavioral context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code'),\n",
              " PosixPath('/Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code/derived/ast_features'),\n",
              " PosixPath('/Users/mahikacalyanakoti/Downloads/College/Year4/Year4Sem1/ESE5460/bat-llm/starter_code/annotations_10k.csv'))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Paths (assume this notebook is run from starter_code/)\n",
        "ROOT = Path.cwd().resolve()\n",
        "DERIVED_DIR = ROOT / 'derived'\n",
        "AST_DIR = DERIVED_DIR / 'ast_features'\n",
        "ANNOT_PATH = ROOT / 'annotations_10k.csv'\n",
        "\n",
        "ROOT, AST_DIR, ANNOT_PATH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load annotations\n",
        "\n",
        "We load the 10k annotations file and sanity-check that the key columns exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emitter</th>\n",
              "      <th>File Name</th>\n",
              "      <th>FileID</th>\n",
              "      <th>Addressee</th>\n",
              "      <th>Context</th>\n",
              "      <th>Emitter pre-vocalization action</th>\n",
              "      <th>Addressee pre-vocalization action</th>\n",
              "      <th>Emitter post-vocalization action</th>\n",
              "      <th>Addressee post-vocalization action</th>\n",
              "      <th>Start sample</th>\n",
              "      <th>End sample</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>216</td>\n",
              "      <td>69809.wav</td>\n",
              "      <td>233219</td>\n",
              "      <td>221</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>590672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>215</td>\n",
              "      <td>71889.wav</td>\n",
              "      <td>237330</td>\n",
              "      <td>220</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>328528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>216</td>\n",
              "      <td>46690.wav</td>\n",
              "      <td>173649</td>\n",
              "      <td>231</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>467792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>230</td>\n",
              "      <td>85411.wav</td>\n",
              "      <td>268012</td>\n",
              "      <td>221</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>475984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>215</td>\n",
              "      <td>45609.wav</td>\n",
              "      <td>170616</td>\n",
              "      <td>220</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>336720</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Emitter  File Name  FileID  Addressee  Context  \\\n",
              "0      216  69809.wav  233219        221       11   \n",
              "1      215  71889.wav  237330        220       12   \n",
              "2      216  46690.wav  173649        231       12   \n",
              "3      230  85411.wav  268012        221       12   \n",
              "4      215  45609.wav  170616        220       12   \n",
              "\n",
              "   Emitter pre-vocalization action  Addressee pre-vocalization action  \\\n",
              "0                                2                                  3   \n",
              "1                                2                                  2   \n",
              "2                                2                                  2   \n",
              "3                                2                                  2   \n",
              "4                                2                                  2   \n",
              "\n",
              "   Emitter post-vocalization action  Addressee post-vocalization action  \\\n",
              "0                                 3                                   3   \n",
              "1                                 3                                   3   \n",
              "2                                 3                                   3   \n",
              "3                                 3                                   3   \n",
              "4                                 3                                   3   \n",
              "\n",
              "   Start sample  End sample  \n",
              "0             1      590672  \n",
              "1             1      328528  \n",
              "2             1      467792  \n",
              "3             1      475984  \n",
              "4             1      336720  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_annotations(path: Path) -> pd.DataFrame:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f'annotations file not found at {path}')\n",
        "    ann = pd.read_csv(path)\n",
        "    # Expect at least these columns:\n",
        "    # Emitter, File Name, FileID, Addressee, Context,\n",
        "    # Emitter pre-vocalization action, Addressee pre-vocalization action,\n",
        "    # Emitter post-vocalization action, Addressee post-vocalization action,\n",
        "    # Start sample, End sample\n",
        "    required_cols = ['Emitter', 'File Name', 'Context']\n",
        "    missing = [c for c in required_cols if c not in ann.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f'annotations_10k.csv missing columns: {missing}')\n",
        "    return ann\n",
        "\n",
        "ann = load_annotations(ANNOT_PATH)\n",
        "ann.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect AST features\n",
        "\n",
        "Each file has a pooled AST embedding saved as `derived/ast_features/ast_<stem>.npy`.\n",
        "We load these and align them with the labels from the annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded AST embeddings for 10000 examples (skipped 0 without features); dim=768\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((10000, 768), 10000, 10000)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def collect_ast_features(ann: pd.DataFrame) -> tuple[np.ndarray, list[str], list[str]]:\n",
        "    \"\"\"Load one AST embedding per file and align with labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : np.ndarray, shape (N, D)\n",
        "        AST pooled embeddings.\n",
        "    emitters : list[str]\n",
        "        Emitter labels per example.\n",
        "    contexts : list[str]\n",
        "        Context labels per example.\n",
        "    \"\"\"\n",
        "\n",
        "    stems = ann['File Name'].apply(lambda s: Path(str(s)).stem)\n",
        "\n",
        "    X_list: list[np.ndarray] = []\n",
        "    emitters: list[str] = []\n",
        "    contexts: list[str] = []\n",
        "\n",
        "    missing = 0\n",
        "    for fn, stem, emitter, ctx in zip(ann['File Name'], stems, ann['Emitter'], ann['Context']):\n",
        "        ast_path = AST_DIR / f'ast_{stem}.npy'\n",
        "        if not ast_path.exists():\n",
        "            missing += 1\n",
        "            continue\n",
        "        vec = np.load(ast_path)\n",
        "        # Expect 1D vector; ensure that.\n",
        "        vec = np.asarray(vec, dtype=np.float32).reshape(-1)\n",
        "        X_list.append(vec)\n",
        "        emitters.append(str(emitter))\n",
        "        contexts.append(str(ctx))\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\n",
        "            'No AST feature files found. Make sure 05_Tokenization_Strategies.ipynb '\n",
        "            'has been run to generate derived/ast_features/ast_*.npy.'\n",
        "        )\n",
        "\n",
        "    X = np.vstack(X_list)\n",
        "    print(\n",
        "        f'Loaded AST embeddings for {X.shape[0]} examples '\n",
        "        f'(skipped {missing} without features); dim={X.shape[1]}'\n",
        "    )\n",
        "    return X, emitters, contexts\n",
        "\n",
        "X_ast, y_emitters, y_contexts = collect_ast_features(ann)\n",
        "X_ast.shape, len(y_emitters), len(y_contexts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper to train a simple baseline\n",
        "\n",
        "We use logistic regression on top of standardized AST features and report standard\n",
        "classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_baseline(\n",
        "    X: np.ndarray,\n",
        "    y: list[str],\n",
        "    task_name: str,\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 42,\n",
        ") -> None:\n",
        "    \"\"\"Train a simple logistic-regression baseline and print metrics.\"\"\"\n",
        "\n",
        "    y_arr = np.asarray(y, dtype=object)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y_arr)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y_enc,\n",
        "        test_size=test_size,\n",
        "        stratify=y_enc,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
        "    clf.fit(X_train_s, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test_s)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Baseline: {task_name} (AST + logistic regression)\")\n",
        "\n",
        "    # `LabelEncoder.classes_` is guaranteed to be set after `fit`\n",
        "    classes = le.classes_\n",
        "    if classes is None:\n",
        "        raise RuntimeError(\"LabelEncoder.classes_ is None; encoder was not fitted correctly.\")\n",
        "\n",
        "    print(\"Classes:\", list(classes))\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=classes))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "    print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline 1 – Emitter classification\n",
        "\n",
        "Predict which bat (emitter) produced each call from its AST embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Baseline: Emitter classification (AST + logistic regression)\n",
            "Classes: ['111', '210', '211', '215', '216', '220', '226', '228', '230', '231']\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         111       0.76      0.71      0.74       200\n",
            "         210       0.46      0.42      0.44       200\n",
            "         211       0.46      0.49      0.47       200\n",
            "         215       0.53      0.51      0.52       200\n",
            "         216       0.54      0.62      0.58       200\n",
            "         220       0.52      0.54      0.53       200\n",
            "         226       0.82      0.82      0.82       200\n",
            "         228       0.84      0.81      0.83       200\n",
            "         230       0.68      0.65      0.66       200\n",
            "         231       0.65      0.66      0.66       200\n",
            "\n",
            "    accuracy                           0.62      2000\n",
            "   macro avg       0.63      0.62      0.62      2000\n",
            "weighted avg       0.63      0.62      0.62      2000\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[143   2   9   6  12   2  10   9   3   4]\n",
            " [  0  85  10  27  13  31   0   2  22  10]\n",
            " [  7   6  98  10  33  13   3   1   2  27]\n",
            " [  4  37  15 101   3  24   2   0   8   6]\n",
            " [ 13   2  29   6 124   4   0   1   5  16]\n",
            " [  1  29  15  24   6 108   0   0  14   3]\n",
            " [  8   0   4   0   5   0 165  16   0   2]\n",
            " [  9   0   4   1   3   0  19 163   1   0]\n",
            " [  1  19   2  15   9  20   1   1 129   3]\n",
            " [  2   4  27   2  21   6   0   1   5 132]]\n"
          ]
        }
      ],
      "source": [
        "run_baseline(X_ast, y_emitters, task_name='Emitter classification')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline 2 – Context classification\n",
        "\n",
        "Predict the behavioral context label from the same AST embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Baseline: Context classification (AST + logistic regression)\n",
            "Classes: ['0', '1', '10', '11', '12', '2', '3', '4', '5', '6', '7', '9']\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         8\n",
            "           1       1.00      1.00      1.00         3\n",
            "          10       0.08      0.11      0.09        35\n",
            "          11       0.64      0.58      0.61       552\n",
            "          12       0.74      0.80      0.77       979\n",
            "           2       0.11      0.07      0.09        84\n",
            "           3       0.25      0.28      0.27        74\n",
            "           4       0.03      0.04      0.03        27\n",
            "           5       0.00      0.00      0.00        12\n",
            "           6       0.91      0.86      0.88       145\n",
            "           7       0.00      0.00      0.00        14\n",
            "           9       0.32      0.33      0.33        67\n",
            "\n",
            "    accuracy                           0.64      2000\n",
            "   macro avg       0.34      0.34      0.34      2000\n",
            "weighted avg       0.63      0.64      0.64      2000\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[  0   0   0   4   1   0   1   1   0   0   0   1]\n",
            " [  0   3   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   4  16   7   2   5   1   0   0   0   0]\n",
            " [  0   0  32 322 129   6  39   9   0   7   1   7]\n",
            " [  0   0   9  89 780  36  14  17   3   3   8  20]\n",
            " [  0   0   4   4  59   6   0   0   0   0   1  10]\n",
            " [  0   0   0  32  10   1  21   1   0   2   0   7]\n",
            " [  0   0   1  12  13   0   0   1   0   0   0   0]\n",
            " [  0   0   0   0   9   1   1   0   0   0   0   1]\n",
            " [  0   0   0  14   6   0   1   0   0 124   0   0]\n",
            " [  0   0   0   2  11   1   0   0   0   0   0   0]\n",
            " [  1   0   0   5  32   4   1   1   0   1   0  22]]\n"
          ]
        }
      ],
      "source": [
        "if len(set(y_contexts)) > 1:\n",
        "    run_baseline(X_ast, y_contexts, task_name='Context classification')\n",
        "else:\n",
        "    print('[info] Skipping context baseline: only one unique context label found.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
