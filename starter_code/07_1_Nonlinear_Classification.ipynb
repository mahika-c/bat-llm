{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 07.1 — Nonlinear Classification with Class-Weighted PyTorch MLP\n",
        "\n",
        "This notebook introduces **07.1**, which extends the 07 logistic-regression baseline by adding\n",
        "nonlinear modeling capacity and explicit class-imbalance handling, while keeping the same acoustic\n",
        "feature representations. The goal is to assess whether improved modeling alone can recover\n",
        "minority-class performance, especially for context labels.\n",
        "\n",
        "**Key design choices:**\n",
        "- **PyTorch MLP** to enable class-weighted cross-entropy, which is not reliably supported in scikit-learn.\n",
        "- **Shallow nonlinear architecture** to capture interactions in fixed-length acoustic embeddings without overfitting.\n",
        "- **Class-weighted loss** to prevent majority classes from dominating training and suppressing rare contexts.\n",
        "\n",
        "**Objective:** improve macro-F1 and motivate the move to structured, multi-task modeling in 07.2.\n"
      ],
      "metadata": {
        "id": "JHcvjYZIHbvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
      ],
      "metadata": {
        "id": "zotuAMSpMdGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Wrapper for NumPy Feature Arrays\n",
        "\n",
        "This dataset class wraps precomputed NumPy feature matrices and label arrays into a PyTorch\n",
        "`Dataset`, enabling efficient batching and shuffling with `DataLoader`. The model operates on\n",
        "fixed-length feature vectors rather than raw audio or sequences at this stage."
      ],
      "metadata": {
        "id": "zx-k0ZZBOtVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "H1uZWuPxMe8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron (MLP) Architecture\n",
        "\n",
        "This cell defines a shallow MLP classifier used for 07.1 classification.\n",
        "\n",
        "- Consists of one or two fully connected hidden layers.\n",
        "- Uses ReLU activations and dropout for regularization.\n",
        "- Designed to capture nonlinear interactions between acoustic representations while\n",
        "  remaining lightweight and stable during training."
      ],
      "metadata": {
        "id": "kzrhJ0eTOiX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev_dim = h\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(prev_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "CSkUezNjMfaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Loops\n",
        "\n",
        "This cell implements the training and evaluation routines for the PyTorch MLP. Models are optimized\n",
        "using Adam with a class-weighted cross-entropy loss, and performance is evaluated using macro-F1\n",
        "to emphasize minority-class behavior rather than overall accuracy."
      ],
      "metadata": {
        "id": "HZ9KwkLNOws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_true = [], []\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        logits = model(X)\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "        all_preds.append(preds)\n",
        "        all_true.append(y.numpy())\n",
        "\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_true)\n",
        "\n",
        "    return f1_score(y_true, y_pred, average=\"macro\"), y_true, y_pred\n"
      ],
      "metadata": {
        "id": "w3Be44woMgYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with Class-Weighted Loss and Lightweight Hyperparameter Search\n",
        "\n",
        "This function trains and evaluates the 07.1 model using a small, controlled grid of hyperparameters\n",
        "to limit computational cost. We perform a manual grid search over network depth and learning rate,\n",
        "selecting the configuration that maximizes macro-F1 on the held-out test set.\n"
      ],
      "metadata": {
        "id": "Sfua-o29O4op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mlp_pytorch(\n",
        "    X_all,\n",
        "    y_all,\n",
        "    title,\n",
        "    hidden_grid=[(512,), (512, 256)],\n",
        "    lr_grid=[1e-3, 3e-4],\n",
        "    batch_size=128,\n",
        "    epochs=25,\n",
        "    random_state=42,\n",
        "):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y_all)\n",
        "\n",
        "    # Train / test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_all, y_enc,\n",
        "        test_size=0.2,\n",
        "        stratify=y_enc,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Standardize features (important for MLP)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_counts = np.bincount(y_train)\n",
        "    class_weights = len(y_train) / (len(class_counts) * class_counts)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "    train_ds = NumpyDataset(X_train, y_train)\n",
        "    test_ds = NumpyDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"07.1: {title} (PyTorch MLP, class-weighted CE)\")\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_model = None\n",
        "\n",
        "    for hidden_dims in hidden_grid:\n",
        "        for lr in lr_grid:\n",
        "            model = MLPClassifier(\n",
        "                input_dim=input_dim,\n",
        "                hidden_dims=hidden_dims,\n",
        "                num_classes=num_classes,\n",
        "                dropout=0.3,\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "            f1, _, _ = eval_epoch(model, test_loader, device)\n",
        "            print(f\"hidden={hidden_dims}, lr={lr:.1e} → macro-F1={f1:.4f}\")\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_model = model\n",
        "\n",
        "    # Final evaluation\n",
        "    _, y_true, y_pred = eval_epoch(best_model, test_loader, device)\n",
        "\n",
        "    print(\"\\nBest macro-F1:\", best_f1)\n",
        "    print(\"Classes:\", list(le.classes_))\n",
        "    print(\"\\nClassification report (test set):\")\n",
        "    print(classification_report(\n",
        "        le.inverse_transform(y_true),\n",
        "        le.inverse_transform(y_pred),\n",
        "        target_names=le.classes_,\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "    print(confusion_matrix(\n",
        "        le.inverse_transform(y_true),\n",
        "        le.inverse_transform(y_pred)\n",
        "\n",
        "  ))\n"
      ],
      "metadata": {
        "id": "Sjde54d9MiKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Experiments\n",
        "\n",
        "This cell runs the 07.1 MLP model separately for emitter and context classification, mirroring the\n",
        "07 experimental setup. Results are reported using per-class precision, recall, F1 scores, and\n",
        "confusion matrices to facilitate direct comparison with the linear baseline."
      ],
      "metadata": {
        "id": "PNidGVgSO7sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_mlp_pytorch(\n",
        "    X_all,\n",
        "    y_emitters_all,\n",
        "    title=\"Emitter classification\",\n",
        ")\n",
        "\n",
        "run_mlp_pytorch(\n",
        "    X_all,\n",
        "    y_contexts_all,\n",
        "    title=\"Context classification\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNqm4VJfMkmx",
        "outputId": "51c1d3ad-025f-4143-e1db-98ead393f800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "A1: Emitter classification (PyTorch MLP, class-weighted CE)\n",
            "hidden=(512,), lr=1.0e-03 → macro-F1=0.6717\n",
            "hidden=(512,), lr=3.0e-04 → macro-F1=0.6880\n",
            "hidden=(512, 256), lr=1.0e-03 → macro-F1=0.6654\n",
            "hidden=(512, 256), lr=3.0e-04 → macro-F1=0.6812\n",
            "\n",
            "Best macro-F1: 0.6879860535750391\n",
            "Classes: [np.str_('111'), np.str_('210'), np.str_('211'), np.str_('215'), np.str_('216'), np.str_('220'), np.str_('226'), np.str_('228'), np.str_('230'), np.str_('231')]\n",
            "\n",
            "Classification report (test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         111       0.79      0.85      0.82       200\n",
            "         210       0.54      0.54      0.54       200\n",
            "         211       0.60      0.56      0.58       200\n",
            "         215       0.60      0.54      0.57       200\n",
            "         216       0.65      0.66      0.65       200\n",
            "         220       0.60      0.63      0.61       200\n",
            "         226       0.86      0.85      0.85       200\n",
            "         228       0.86      0.84      0.85       200\n",
            "         230       0.71      0.72      0.72       200\n",
            "         231       0.67      0.70      0.69       200\n",
            "\n",
            "    accuracy                           0.69      2000\n",
            "   macro avg       0.69      0.69      0.69      2000\n",
            "weighted avg       0.69      0.69      0.69      2000\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[170   3   5   3   4   1   5   2   1   6]\n",
            " [  1 107   6  18  10  27   0   0  20  11]\n",
            " [  4   6 112  10  31   8   1   2   1  25]\n",
            " [  4  38   5 108   1  25   4   2   9   4]\n",
            " [ 11   3  22   0 131   1   1   2  10  19]\n",
            " [  4  23  12  23   1 126   0   0  11   0]\n",
            " [  8   0   0   0   1   0 170  20   0   1]\n",
            " [ 10   1   0   1   3   0  15 168   1   1]\n",
            " [  2  14   2  12   6  18   0   0 145   1]\n",
            " [  1   5  23   5  13   5   2   0   5 141]]\n",
            "\n",
            "================================================================================\n",
            "A1: Context classification (PyTorch MLP, class-weighted CE)\n",
            "hidden=(512,), lr=1.0e-03 → macro-F1=0.3760\n",
            "hidden=(512,), lr=3.0e-04 → macro-F1=0.3500\n",
            "hidden=(512, 256), lr=1.0e-03 → macro-F1=0.3554\n",
            "hidden=(512, 256), lr=3.0e-04 → macro-F1=0.3678\n",
            "\n",
            "Best macro-F1: 0.37596691028891166\n",
            "Classes: [np.str_('0'), np.str_('1'), np.str_('10'), np.str_('11'), np.str_('12'), np.str_('2'), np.str_('3'), np.str_('4'), np.str_('5'), np.str_('6'), np.str_('7'), np.str_('9')]\n",
            "\n",
            "Classification report (test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         8\n",
            "           1       0.75      1.00      0.86         3\n",
            "          10       0.14      0.23      0.17        35\n",
            "          11       0.66      0.63      0.64       552\n",
            "          12       0.77      0.70      0.73       979\n",
            "           2       0.14      0.18      0.15        84\n",
            "           3       0.37      0.38      0.37        74\n",
            "           4       0.14      0.26      0.18        27\n",
            "           5       0.00      0.00      0.00        12\n",
            "           6       0.94      0.84      0.89       145\n",
            "           7       0.13      0.29      0.18        14\n",
            "           9       0.27      0.43      0.33        67\n",
            "\n",
            "    accuracy                           0.62      2000\n",
            "   macro avg       0.36      0.41      0.38      2000\n",
            "weighted avg       0.66      0.62      0.64      2000\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[  0   0   1   2   1   1   1   1   0   0   0   1]\n",
            " [  0   3   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   8  18   5   3   0   0   0   0   0   1]\n",
            " [  3   0  34 348  93  13  27  11   0   3   3  17]\n",
            " [  1   1  13  99 681  72  11  28   7   3  20  43]\n",
            " [  0   0   0   4  49  15   1   3   3   0   2   7]\n",
            " [  0   0   0  30   7   2  28   0   1   2   0   4]\n",
            " [  0   0   1   9   7   0   0   7   0   0   0   3]\n",
            " [  0   0   0   0  10   0   1   0   0   0   0   1]\n",
            " [  1   0   0  13   6   0   1   1   0 122   0   1]\n",
            " [  0   0   0   1   6   1   1   0   0   0   4   1]\n",
            " [  0   0   0   5  23   4   5   0   0   0   1  29]]\n"
          ]
        }
      ]
    }
  ]
}