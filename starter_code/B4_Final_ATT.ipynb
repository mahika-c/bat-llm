{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# B4: Audio to Text Generation (Feature-conditioned GRU decoder)\n",
        "\n",
        "**Goal:** Train a neural generator that maps a single bat call’s acoustic features (AST + token histograms) to a short, grammatical natural-language description of the interaction.\n",
        "\n",
        "**Why B4 (after previous model designs):**\n",
        "- Multi-task Structured Classification learned a strong structured semantic representation (emitter/addressee/context/actions) but outputs discrete labels.\n",
        "- B3 showed we can convert labels into readable sentences, but it’s still classification + templating.\n",
        "- B4 learns **direct sentence generation** from audio features, producing an interpretable text output while preserving your controlled, factual structure.\n",
        "\n",
        "**Model design:**\n",
        "- Encoder: small MLP maps 1152-dim audio features → a context vector.\n",
        "- Decoder: GRU generates a sentence token-by-token (teacher forcing training).\n",
        "- This is intentionally simple (interpretable, fast, easy to debug) and matches your dataset scale."
      ],
      "metadata": {
        "id": "uvyASX1HvBSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_MAP = {\n",
        "    \"0\": None,\n",
        "    \"1\": \"separation\",\n",
        "    \"2\": \"biting\",\n",
        "    \"3\": \"feeding\",\n",
        "    \"4\": \"fighting\",\n",
        "    \"5\": \"grooming\",\n",
        "    \"6\": \"isolation\",\n",
        "    \"7\": \"kissing\",\n",
        "    \"8\": \"landing\",\n",
        "    \"9\": \"mating protest\",\n",
        "    \"10\": \"threat-like interaction\",\n",
        "    \"11\": \"general interaction\",\n",
        "    \"12\": \"sleeping\",\n",
        "}\n",
        "\n",
        "PRE_ACTION_MAP = {\n",
        "    \"0\": None,\n",
        "    \"1\": \"flying in\",\n",
        "    \"2\": \"present\",\n",
        "    \"3\": \"crawling in\",\n",
        "}\n",
        "\n",
        "POST_ACTION_MAP = {\n",
        "    \"0\": None,\n",
        "    \"1\": \"cowered\",\n",
        "    \"2\": \"flew away\",\n",
        "    \"3\": \"stayed\",\n",
        "    \"4\": \"crawled away\",\n",
        "}"
      ],
      "metadata": {
        "id": "FoI7uuQPwmU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "mCTjyEc-v0aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def with_article(phrase: str) -> str:\n",
        "    \"\"\"Adds 'a' or 'an' where appropriate.\"\"\"\n",
        "    if phrase is None:\n",
        "        return \"an unspecified interaction\"\n",
        "    if phrase[0].lower() in \"aeiou\":\n",
        "        return f\"an {phrase}\"\n",
        "    return f\"a {phrase}\"\n",
        "\n",
        "def normalize_key(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    # handle pandas/numpy scalars\n",
        "    s = str(x).strip()\n",
        "\n",
        "    # convert \"11.0\" -> \"11\"\n",
        "    try:\n",
        "        f = float(s)\n",
        "        if f.is_integer():\n",
        "            return str(int(f))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # also handle \"11\" already\n",
        "    return s\n",
        "\n",
        "def safe_lookup(mapping: dict, key: str) -> str | None:\n",
        "    k = normalize_key(key)\n",
        "    return mapping.get(str(k), None)"
      ],
      "metadata": {
        "id": "b9Tdkuuhwox4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-r4uRrfv1sF",
        "outputId": "9740e49c-fe9b-4da3-cd23-c933e133f6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def _load_ast_vector(stem: str) -> np.ndarray | None:\n",
        "    ast_path = AST_DIR / f\"ast_{stem}.npy\"\n",
        "    if not ast_path.exists():\n",
        "        return None\n",
        "    vec = np.load(ast_path)\n",
        "    return np.asarray(vec, dtype=np.float32).reshape(-1)\n",
        "\n",
        "def _load_kmeans_hist(stem: str, n_clusters: int = 128) -> np.ndarray | None:\n",
        "    tok_path = KMEANS_DIR / f\"w2v_kmeans_{stem}.npy\"\n",
        "    if not tok_path.exists():\n",
        "        return None\n",
        "    tokens = np.load(tok_path).astype(int)\n",
        "    hist = np.bincount(tokens, minlength=n_clusters).astype(np.float32)\n",
        "    total = hist.sum()\n",
        "    if total > 0:\n",
        "        hist /= total\n",
        "    return hist\n",
        "\n",
        "def _load_vqvae_hist(stem: str, n_codes: int = 256) -> np.ndarray | None:\n",
        "    tok_path = VQ_TOKENS_DIR / f\"vqvae_{stem}.npy\"\n",
        "    if not tok_path.exists():\n",
        "        return None\n",
        "    tokens = np.load(tok_path).astype(int)\n",
        "    hist = np.bincount(tokens, minlength=n_codes).astype(np.float32)\n",
        "    total = hist.sum()\n",
        "    if total > 0:\n",
        "        hist /= total\n",
        "    return hist"
      ],
      "metadata": {
        "id": "wZ1BOF9Yv3QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_features_and_labels_B6(\n",
        "    ann: pd.DataFrame,\n",
        "    use_ast: bool = True,\n",
        "    use_kmeans_tokens: bool = True,\n",
        "    use_vqvae_tokens: bool = True,\n",
        ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Builds X (N, 1152) and returns a *filtered* ann_aligned dataframe with the same row order as X.\n",
        "    \"\"\"\n",
        "    stems = ann[\"File Name\"].apply(lambda s: Path(str(s)).stem)\n",
        "\n",
        "    X_list: List[np.ndarray] = []\n",
        "    rows: List[int] = []\n",
        "\n",
        "    missing_any = 0\n",
        "\n",
        "    for i, (stem,) in enumerate(zip(stems)):\n",
        "        parts: List[np.ndarray] = []\n",
        "\n",
        "        if use_ast:\n",
        "            ast_vec = _load_ast_vector(stem)\n",
        "            if ast_vec is None:\n",
        "                missing_any += 1\n",
        "                continue\n",
        "            parts.append(ast_vec)\n",
        "\n",
        "        if use_kmeans_tokens:\n",
        "            km_hist = _load_kmeans_hist(stem, n_clusters=128)\n",
        "            if km_hist is None:\n",
        "                missing_any += 1\n",
        "                continue\n",
        "            parts.append(km_hist)\n",
        "\n",
        "        if use_vqvae_tokens:\n",
        "            vq_hist = _load_vqvae_hist(stem, n_codes=256)  # IMPORTANT: fixed 256\n",
        "            if vq_hist is None:\n",
        "                missing_any += 1\n",
        "                continue\n",
        "            parts.append(vq_hist)\n",
        "\n",
        "        feat_vec = np.concatenate(parts).astype(np.float32)\n",
        "        X_list.append(feat_vec)\n",
        "        rows.append(i)\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No feature vectors constructed.\")\n",
        "\n",
        "    X = np.vstack(X_list)\n",
        "    ann_aligned = ann.iloc[rows].reset_index(drop=True)\n",
        "\n",
        "    print(f\"Built X for {X.shape[0]} examples; dim={X.shape[1]}. Skipped {missing_any}.\")\n",
        "    return X, ann_aligned"
      ],
      "metadata": {
        "id": "fO1VWDByv4vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_or_unknown(m: Dict[str, str], key: str) -> str:\n",
        "    key = str(key)\n",
        "    return m[key] if key in m else f\"unknown({key})\"\n"
      ],
      "metadata": {
        "id": "EpcUk_2Nv8RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_caption_from_row(row: pd.Series) -> str:\n",
        "    emitter = str(row[\"Emitter\"])\n",
        "    addressee = str(row[\"Addressee\"])\n",
        "\n",
        "    context = safe_lookup(CONTEXT_MAP, row[\"Context\"])\n",
        "    e_pre = safe_lookup(PRE_ACTION_MAP, row[\"Emitter pre-vocalization action\"])\n",
        "    a_pre = safe_lookup(PRE_ACTION_MAP, row[\"Addressee pre-vocalization action\"])\n",
        "    e_post = safe_lookup(POST_ACTION_MAP, row[\"Emitter post-vocalization action\"])\n",
        "    a_post = safe_lookup(POST_ACTION_MAP, row[\"Addressee post-vocalization action\"])\n",
        "\n",
        "    # Sentence 1: core event\n",
        "    s1 = (\n",
        "        f\"Bat {emitter} vocalized toward bat {addressee} \"\n",
        "        f\"during {with_article(context)}.\"\n",
        "    )\n",
        "\n",
        "    # Sentence 2: pre-vocalization\n",
        "    if e_pre or a_pre:\n",
        "        pre_parts = []\n",
        "        if e_pre:\n",
        "            pre_parts.append(f\"the emitter was {e_pre}\")\n",
        "        if a_pre:\n",
        "            pre_parts.append(f\"the addressee was {a_pre}\")\n",
        "        s2 = \"Before vocalizing, \" + \" and \".join(pre_parts) + \".\"\n",
        "    else:\n",
        "        s2 = \"Before vocalizing, no clear movement was observed.\"\n",
        "\n",
        "    # Sentence 3: post-vocalization\n",
        "    if e_post or a_post:\n",
        "        post_parts = []\n",
        "        if e_post:\n",
        "            post_parts.append(f\"the emitter {e_post}\")\n",
        "        if a_post:\n",
        "            post_parts.append(f\"the addressee {a_post}\")\n",
        "        s3 = \"Afterward, \" + \" while \".join(post_parts) + \".\"\n",
        "    else:\n",
        "        s3 = \"Afterward, no clear movement was observed.\"\n",
        "\n",
        "    return \" \".join([s1, s2, s3])\n",
        "\n",
        "def build_all_captions(ann_aligned: pd.DataFrame) -> List[str]:\n",
        "    return [build_caption_from_row(ann_aligned.iloc[i]) for i in range(len(ann_aligned))]\n"
      ],
      "metadata": {
        "id": "DodClPQxv93K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
        "PAD, BOS, EOS, UNK = SPECIAL\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    # conservative tokenizer\n",
        "    text = text.lower().strip()\n",
        "    for ch in [\".\", \",\", \":\", \";\"]:\n",
        "        text = text.replace(ch, f\" {ch} \")\n",
        "    return text.split()\n",
        "\n",
        "@dataclass\n",
        "class Vocab:\n",
        "    stoi: Dict[str, int]\n",
        "    itos: List[str]\n",
        "\n",
        "    @property\n",
        "    def pad_id(self): return self.stoi[PAD]\n",
        "    @property\n",
        "    def bos_id(self): return self.stoi[BOS]\n",
        "    @property\n",
        "    def eos_id(self): return self.stoi[EOS]\n",
        "    @property\n",
        "    def unk_id(self): return self.stoi[UNK]\n",
        "\n",
        "def build_vocab(texts: List[str], min_freq: int = 2) -> Vocab:\n",
        "    freq: Dict[str, int] = {}\n",
        "    for t in texts:\n",
        "        for tok in simple_tokenize(t):\n",
        "            freq[tok] = freq.get(tok, 0) + 1\n",
        "\n",
        "    itos = list(SPECIAL)\n",
        "    for tok, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])):\n",
        "        if c >= min_freq and tok not in SPECIAL:\n",
        "            itos.append(tok)\n",
        "\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    return Vocab(stoi=stoi, itos=itos)\n",
        "\n",
        "def encode_text(vocab: Vocab, text: str, max_len: int = 64) -> List[int]:\n",
        "    toks = [BOS] + simple_tokenize(text) + [EOS]\n",
        "    ids = [vocab.stoi.get(tok, vocab.unk_id) for tok in toks]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [vocab.pad_id] * (max_len - len(ids))\n",
        "    else:\n",
        "        ids = ids[:max_len]\n",
        "        ids[-1] = vocab.eos_id\n",
        "    return ids\n",
        "\n",
        "def decode_ids(vocab: Vocab, ids: List[int]) -> str:\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        tok = vocab.itos[i] if 0 <= i < len(vocab.itos) else UNK\n",
        "        if tok in (BOS, PAD):\n",
        "            continue\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        toks.append(tok)\n",
        "    # detokenize lightly\n",
        "    out = \" \".join(toks)\n",
        "    out = out.replace(\" .\", \".\").replace(\" ,\", \",\")\n",
        "    return out"
      ],
      "metadata": {
        "id": "C5KBGg_7v_Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class B6Dataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, captions: List[str], vocab: Vocab, max_len: int = 64):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.tensor([encode_text(vocab, c, max_len=max_len) for c in captions], dtype=torch.long)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def make_loaders(X: np.ndarray, captions: List[str], test_size=0.2, seed=42, batch_size=128, max_len=64):\n",
        "    idx = np.arange(len(X))\n",
        "    tr_idx, te_idx = train_test_split(idx, test_size=test_size, random_state=seed, shuffle=True)\n",
        "\n",
        "    train_texts = [captions[i] for i in tr_idx]\n",
        "    vocab = build_vocab(train_texts, min_freq=2)\n",
        "\n",
        "    ds_tr = B6Dataset(X[tr_idx], [captions[i] for i in tr_idx], vocab, max_len=max_len)\n",
        "    ds_te = B6Dataset(X[te_idx], [captions[i] for i in te_idx], vocab, max_len=max_len)\n",
        "\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    dl_te = DataLoader(ds_te, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    return dl_tr, dl_te, vocab, tr_idx, te_idx"
      ],
      "metadata": {
        "id": "Hf914wkIwA0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class B6Captioner(nn.Module):\n",
        "    def __init__(self, x_dim: int, vocab_size: int, enc_hidden: int = 512,\n",
        "                 dec_hidden: int = 512, emb_dim: int = 256, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder maps X -> context vector\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(x_dim, enc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(enc_hidden, dec_hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=dec_hidden, batch_first=True)\n",
        "        self.out = nn.Linear(dec_hidden, vocab_size)\n",
        "\n",
        "    def forward(self, X: torch.Tensor, y_inp: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Teacher forcing:\n",
        "          - y_inp: (B, T) token ids, includes BOS... (we predict next token)\n",
        "        \"\"\"\n",
        "        h0 = self.encoder(X).unsqueeze(0)         # (1, B, H)\n",
        "        emb = self.emb(y_inp)                     # (B, T, E)\n",
        "        out, _ = self.gru(emb, h0)                # (B, T, H)\n",
        "        logits = self.out(out)                    # (B, T, V)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, X: torch.Tensor, bos_id: int, eos_id: int, max_len: int = 64):\n",
        "        B = X.size(0)\n",
        "        h = self.encoder(X).unsqueeze(0)\n",
        "\n",
        "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=X.device)\n",
        "\n",
        "        for _ in range(max_len - 1):\n",
        "            emb = self.emb(ys[:, -1:])            # (B, 1, E)\n",
        "            out, h = self.gru(emb, h)             # (B, 1, H)\n",
        "            logits = self.out(out[:, -1, :])      # (B, V)\n",
        "            nxt = torch.argmax(logits, dim=-1, keepdim=True)  # greedy\n",
        "            ys = torch.cat([ys, nxt], dim=1)\n",
        "            if (nxt.squeeze(1) == eos_id).all():\n",
        "                break\n",
        "        return ys"
      ],
      "metadata": {
        "id": "mjtW-9spwClo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_ce_loss(logits: torch.Tensor, targets: torch.Tensor, pad_id: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    logits: (B, T, V)\n",
        "    targets: (B, T)  (the desired next tokens aligned with logits)\n",
        "    \"\"\"\n",
        "    B, T, V = logits.shape\n",
        "    logits = logits.reshape(B*T, V)\n",
        "    targets = targets.reshape(B*T)\n",
        "    return F.cross_entropy(logits, targets, ignore_index=pad_id)\n",
        "\n",
        "@torch.no_grad()\n",
        "def token_accuracy(pred: torch.Tensor, gold: torch.Tensor, pad_id: int) -> float:\n",
        "    mask = (gold != pad_id)\n",
        "    if mask.sum().item() == 0:\n",
        "        return 0.0\n",
        "    return (pred[mask] == gold[mask]).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def exact_match(pred: torch.Tensor, gold: torch.Tensor, pad_id: int) -> float:\n",
        "    # exact match over non-pad positions\n",
        "    B = pred.size(0)\n",
        "    matches = 0\n",
        "    for i in range(B):\n",
        "        g = gold[i]\n",
        "        p = pred[i]\n",
        "        g_len = (g != pad_id).sum().item()\n",
        "        if g_len == 0:\n",
        "            continue\n",
        "        if torch.equal(p[:g_len], g[:g_len]):\n",
        "            matches += 1\n",
        "    return matches / B\n",
        "\n",
        "def train_one_config(\n",
        "    dl_tr, dl_te, vocab: Vocab,\n",
        "    x_dim: int = 1152,\n",
        "    enc_hidden: int = 512,\n",
        "    dec_hidden: int = 512,\n",
        "    lr: float = 1e-3,\n",
        "    dropout: float = 0.2,\n",
        "    epochs: int = 12,\n",
        "):\n",
        "    model = B6Captioner(x_dim=x_dim, vocab_size=len(vocab.itos),\n",
        "                        enc_hidden=enc_hidden, dec_hidden=dec_hidden,\n",
        "                        emb_dim=256, dropout=dropout).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for Xb, yb in dl_tr:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            # teacher forcing: input tokens are y[:, :-1], predict y[:, 1:]\n",
        "            y_inp = yb[:, :-1]\n",
        "            y_tgt = yb[:, 1:]\n",
        "\n",
        "            logits = model(Xb, y_inp)\n",
        "            loss = seq_ce_loss(logits, y_tgt, pad_id=vocab.pad_id)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item() * Xb.size(0)\n",
        "\n",
        "        if ep in (1, 3, 6, 9, epochs):\n",
        "            model.eval()\n",
        "            accs = []\n",
        "            ems = []\n",
        "            for Xb, yb in dl_te:\n",
        "                Xb = Xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "\n",
        "                gen = model.generate(Xb, bos_id=vocab.bos_id, eos_id=vocab.eos_id, max_len=yb.size(1))\n",
        "                # compare generated tokens to full target sequence\n",
        "                # align lengths: gen includes BOS; yb includes BOS. keep same shape by pad/truncate\n",
        "                if gen.size(1) < yb.size(1):\n",
        "                    pad = torch.full((gen.size(0), yb.size(1)-gen.size(1)), vocab.pad_id, device=device, dtype=torch.long)\n",
        "                    gen = torch.cat([gen, pad], dim=1)\n",
        "                else:\n",
        "                    gen = gen[:, :yb.size(1)]\n",
        "\n",
        "                accs.append(token_accuracy(gen, yb, pad_id=vocab.pad_id))\n",
        "                ems.append(exact_match(gen, yb, pad_id=vocab.pad_id))\n",
        "\n",
        "            print(f\"epoch {ep:02d}/{epochs} train_loss={total/len(dl_tr.dataset):.4f}  \"\n",
        "                  f\"val_token_acc={sum(accs)/len(accs):.4f}  val_exact_match={sum(ems)/len(ems):.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "r1Ajna5GwEkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build features + aligned annotations\n",
        "X_all, ann_aligned = collect_features_and_labels_B6(\n",
        "    ann,\n",
        "    use_ast=True,\n",
        "    use_kmeans_tokens=True,\n",
        "    use_vqvae_tokens=True,\n",
        ")\n",
        "\n",
        "# Build caption targets\n",
        "captions = build_all_captions(ann_aligned)\n",
        "\n",
        "# DataLoaders + vocab\n",
        "dl_tr, dl_te, vocab, tr_idx, te_idx = make_loaders(\n",
        "    X_all, captions,\n",
        "    test_size=0.2,\n",
        "    seed=42,\n",
        "    batch_size=128,\n",
        "    max_len=64\n",
        ")\n",
        "\n",
        "print(\"Vocab size:\", len(vocab.itos))\n",
        "print(\"Example target:\", captions[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRsgwZLjwGnk",
        "outputId": "0c49742b-2c60-4ba4-9489-764549bc21a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built X for 10000 examples; dim=1152. Skipped 0.\n",
            "Vocab size: 76\n",
            "Example target: Bat 216 vocalized toward bat 221 during a general interaction. Before vocalizing, the emitter was present and the addressee was crawling in. Afterward, the emitter stayed while the addressee stayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "    {\"enc_hidden\": 512, \"dec_hidden\": 512, \"lr\": 1e-3, \"dropout\": 0.2},\n",
        "    {\"enc_hidden\": 512, \"dec_hidden\": 512, \"lr\": 3e-4, \"dropout\": 0.2},\n",
        "]\n",
        "\n",
        "best_model = None\n",
        "best_score = -1.0\n",
        "\n",
        "for cfg in configs:\n",
        "    print(\"\\n================================================================================\")\n",
        "    print(\"B6 config:\", cfg)\n",
        "\n",
        "    model = train_one_config(\n",
        "        dl_tr, dl_te, vocab,\n",
        "        x_dim=X_all.shape[1],\n",
        "        enc_hidden=cfg[\"enc_hidden\"],\n",
        "        dec_hidden=cfg[\"dec_hidden\"],\n",
        "        lr=cfg[\"lr\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        epochs=12,\n",
        "    )\n",
        "\n",
        "    # Evaluate token accuracy on test set (quick score)\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    for Xb, yb in dl_te:\n",
        "        Xb = Xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        gen = model.generate(Xb, vocab.bos_id, vocab.eos_id, max_len=yb.size(1))\n",
        "        if gen.size(1) < yb.size(1):\n",
        "            pad = torch.full((gen.size(0), yb.size(1)-gen.size(1)), vocab.pad_id, device=device, dtype=torch.long)\n",
        "            gen = torch.cat([gen, pad], dim=1)\n",
        "        else:\n",
        "            gen = gen[:, :yb.size(1)]\n",
        "        accs.append(token_accuracy(gen, yb, vocab.pad_id))\n",
        "    score = float(sum(accs)/len(accs))\n",
        "    print(f\"B6 token-accuracy (test): {score:.4f}\")\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model = model\n",
        "\n",
        "print(\"\\n################################################################################\")\n",
        "print(\"BEST B6 token-accuracy:\", best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_U7yyYowIcO",
        "outputId": "265f2745-c719-4dde-b692-03443a0b4ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "B6 config: {'enc_hidden': 512, 'dec_hidden': 512, 'lr': 0.001, 'dropout': 0.2}\n",
            "epoch 01/12 train_loss=0.6330  val_token_acc=0.6955  val_exact_match=0.1474\n",
            "epoch 03/12 train_loss=0.1471  val_token_acc=0.7420  val_exact_match=0.1975\n",
            "epoch 06/12 train_loss=0.1252  val_token_acc=0.7526  val_exact_match=0.2103\n",
            "epoch 09/12 train_loss=0.1163  val_token_acc=0.7632  val_exact_match=0.2336\n",
            "epoch 12/12 train_loss=0.1087  val_token_acc=0.7678  val_exact_match=0.2447\n",
            "B6 token-accuracy (test): 0.7678\n",
            "\n",
            "================================================================================\n",
            "B6 config: {'enc_hidden': 512, 'dec_hidden': 512, 'lr': 0.0003, 'dropout': 0.2}\n",
            "epoch 01/12 train_loss=1.3406  val_token_acc=0.6752  val_exact_match=0.0735\n",
            "epoch 03/12 train_loss=0.1780  val_token_acc=0.6873  val_exact_match=0.1552\n",
            "epoch 06/12 train_loss=0.1413  val_token_acc=0.7370  val_exact_match=0.1902\n",
            "epoch 09/12 train_loss=0.1297  val_token_acc=0.7520  val_exact_match=0.1923\n",
            "epoch 12/12 train_loss=0.1233  val_token_acc=0.7565  val_exact_match=0.2355\n",
            "B6 token-accuracy (test): 0.7565\n",
            "\n",
            "################################################################################\n",
            "BEST B6 token-accuracy: 0.7678466029465199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a few qualitative examples\n",
        "best_model.eval()\n",
        "\n",
        "# pick a few indices from test set\n",
        "sample_ids = list(te_idx[:5])\n",
        "for idx in sample_ids:\n",
        "    X = torch.from_numpy(X_all[idx:idx+1]).float().to(device)\n",
        "    gen = best_model.generate(X, vocab.bos_id, vocab.eos_id, max_len=64)[0].tolist()\n",
        "\n",
        "    gt = captions[idx]\n",
        "    pred = decode_ids(vocab, gen)\n",
        "\n",
        "    print(\"\\n================================================================================\")\n",
        "    print(\"Index:\", idx)\n",
        "    print(\"GT  :\", gt)\n",
        "    print(\"PRED:\", pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbObtep6wJYf",
        "outputId": "732ee662-e2fd-42d0-9c9e-e97d0c51e259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Index: 6252\n",
            "GT  : Bat 230 vocalized toward bat 207 during a sleeping. Before vocalizing, the emitter was present and the addressee was present. Afterward, the emitter stayed while the addressee stayed.\n",
            "PRED: bat 230 vocalized toward bat 207 during a sleeping. before vocalizing, the emitter was present and the addressee was present. afterward, the emitter stayed while the addressee stayed.\n",
            "\n",
            "================================================================================\n",
            "Index: 4684\n",
            "GT  : Bat 230 vocalized toward bat 207 during a sleeping. Before vocalizing, the emitter was present and the addressee was present. Afterward, the emitter stayed while the addressee stayed.\n",
            "PRED: bat 230 vocalized toward bat 207 during a sleeping. before vocalizing, the emitter was present and the addressee was present. afterward, the emitter stayed while the addressee stayed.\n",
            "\n",
            "================================================================================\n",
            "Index: 1731\n",
            "GT  : Bat 216 vocalized toward bat 233 during a general interaction. Before vocalizing, no clear movement was observed. Afterward, the emitter flew away while the addressee stayed.\n",
            "PRED: bat 111 vocalized toward bat 228 during a general interaction. before vocalizing, the emitter was present and the addressee was present. afterward, the emitter flew away while the addressee stayed.\n",
            "\n",
            "================================================================================\n",
            "Index: 4742\n",
            "GT  : Bat 230 vocalized toward bat 221 during a general interaction. Before vocalizing, the emitter was present and the addressee was present. Afterward, the emitter stayed while the addressee stayed.\n",
            "PRED: bat 230 vocalized toward bat 207 during a sleeping. before vocalizing, the emitter was present and the addressee was present. afterward, the emitter stayed while the addressee stayed.\n",
            "\n",
            "================================================================================\n",
            "Index: 4521\n",
            "GT  : Bat 211 vocalized toward bat 208 during a general interaction. Before vocalizing, the emitter was present and the addressee was present. Afterward, the emitter stayed while the addressee stayed.\n",
            "PRED: bat 211 vocalized toward bat 208 during a sleeping. before vocalizing, the emitter was present and the addressee was present. afterward, the emitter stayed while the addressee stayed.\n"
          ]
        }
      ]
    }
  ]
}